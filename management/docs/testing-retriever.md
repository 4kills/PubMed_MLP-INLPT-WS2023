# Testing Retriever

## Idea

- We want to check how good different retrieval Pipelines (in Opensearch) are at retrieving relevant documents for a given query-document pair dataset.

- retrieval Pipelines to test:
  - BM25
  - neural search (with s-pubmed-bert embeddings)
  - different weighted hybrid search (BM25 + neural search)

- in these pipelines, the BM25 parts matches on multiple fields of the document, namely the 'abstract-fragment', 'title' and 'keywords' fields. The neural search pipeline only matches on the embedding of the 'abstract-fragment' field, which we named 'abstract-fragment-embedding' in the documents.

- Use test set generated with OpenAi GPT-3.5Turbo [Ref to earlier paragrpah]

- To test this we first take the Question (query) of the test set, then retrieve the most relevant i document abstract-fragments for the given pipelines. Then evaluate on the entire test set how good the retrieval was based on how many of the retrieved documents were the actual ground truth documents from wich the Question was generated.

- For this evaluation we use the following metrics:
    <!-- TODO! -->

---

## Steps

### 1. For this we first need to define different pipelines in Opensearch

- Here we opted for incremental steps of 0.05 from an weight of 0 to 1 for the neural search. Like this we are testing all above mentioned pipelines:
  - -> neural search weight = 0.00 -> BM25
  - -> neural search weight = 0.05 -> hybrid search (with 0.05 neural search and 0.95 BM25)
  - ...
  - -> neural search weight = 1.00 -> neural search

- Since the we didn't find any way of adding these search-pipelines into Opensearch via its Pyhon API, we had to do this manually via the Opensearch dashboard.

  - Our naming convention for the pipelines was:  "hybrid_search_pipeline_weight{neural_search_weight}". E.g. "hybrid_search_pipeline_weight0.05" for a neural search weight of 0.05.

Example Pipeline for a neural search weight of 0.00 (This represents the BM25 retrieval pipeline):

```json
PUT /_search/pipeline/hybrid_search_pipeline_weight_0.00
{
  "description": "Post processor for hybrid search",
  "phase_results_processors": [
    {
      "normalization-processor": {
        "normalization": {
          "technique": "min_max"
        },
        "combination": {
          "technique": "arithmetic_mean",
          "parameters": {
            "weights": [
              0.00, // neural search weight -> increase this in increments of 0.05
              1.00 // BM25 weight -> proportionally decrease this in increments of 0.05
            ]
          }
        }
      }
    }
  ]
}
```

### 2. Adjust the Opensearch Python APi connector libary (written by us) to be able to specify which neural search weight to use for the retrieval pipeline

For this we combine both an **BM25** and a **neural search** query into a single query. Were the above pipleline is used to combine the results of the two queries into a single result. Wheighing the results of the two queries according to the neural search weight.

For the BM25 query, we are able to do the matching on multiple fields of the documents. Fields we for now are matching on are: 'abstract-fragment', 'title' and 'keywords_list'.

```python
def create_multi_match_BM25_query(
    query_text, match_on_fields=["abstract_fragment", "title", "keyword_list"]
):
    return {"multi_match": {"query": query_text, "fields": match_on_fields}}
```

While the neural search query is  matching on the 'abstract-fragment-embedding' field of the documents. This field contains the embeddings of the 'abstract-fragment' generated by the 'pritamdeka/S-PubMedBert-MS-MARCO' model during ingestion. To make this possible we first also embed the query text using the same model.

```python
def create_neural_query(query_text):
    return {
        "neural": {
            "abstract_fragment_embedding": {
                "query_text": query_text,
                "model_id": get_model_id(CLIENT),
            }
        }
    }
```

Both of these queries are then combined into a single hybrid query, which is then used to retrieve the documents.

```python
def create_hybrid_query(
    query_text, match_on_fields=["abstract_fragment", "title", "keyword_list"]
):
    """
    Create a hybrid query that combines BM25 and neural search.
    """
    return {
        "hybrid": {
            "queries": [
                create_neural_query(query_text),
                create_multi_match_BM25_query(query_text, match_on_fields),
            ],
        }
    }
```

When executing the query we then specify the pipeline to use for the retrieval. Thereby also indirectly specifying the neural search weight to use (naming is done with an function and thereby abstracted away; below is just an example).

```python
CLIENT.search(
        body=query_body, # the before mentioned hybrid query
        index="abstracts", # index to search in
        _source_includes=["_id", "fragment_id"], # fields to include in the result
        params={"search_pipeline": "hybrid_search_pipeline_weight_0.05"}, # specify the pipeline
    )
```

### 3. Setup the retrieval script for evaluation

The Idea is to retrieve the top i documents for each query in the test set.



